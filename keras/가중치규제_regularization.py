'''
Regularization : 정규화, overfitting 억제 방법


#### L1 regularization ####
L1 : 가중치 각 요소 절댓값의 합
L1 정규화는 불필요한 피쳐(특성)에 대응하는 가중치를 정확히 '0'으로 만들어
해당 피처(특성)를 모델이 무시하도록 만들어 준다.
한마디로, 변수 선택(feature selection)의 효과가 있다.

각 가중치 절댓값의 합에 규제 강도 람다를 곱하여 오차에 더한다.
이렇게 하면 L2 규제와는 달리 어떤 가중치는 실제로 0이 된다. -> 모델에서 완전히 제외되는 특성이 생김
일부 계수를 0으로 만듦으로써 모델을 이해하기 쉬워지고, 모델의 가장 중요한 특성이 무엇인지 드러난다.




#### L2 regularization ####
L2 : 가중치 각 요소 제곱의 합
L2 정규화는 아주 큰 값이나 작은 값을 가지는 outlier 모델 가중치에 대해 0에 가깝지만 0은 아닌 값으로 만든다.
(0으로 수렴하도록 한다)
L2 정규화는 선형모델의 일반화 능력을 언제나 항상 개선시킨다

각 가중치 제곱의 합에 규제 강도(Regularization Strength) 람다(lambda)를 곱한다.
그 값을 손실함수에 더한다. 람다를 크게 하면 가중치가 더 많이 감소되고(규제 중시), 람다를 작게하면 가중치가 증가한다(규제 중시 x)
가중치를 갱신할 때, 손실함수의 미분값을 이전 가중치에서 빼서 다음 가중치를 계산한다.
따라서 가중치가 크면 손실함수가 커지고, 다음 가중치가 크게 감소된다.


ex) from keras.regularizers import l1, l2, l1_l2    <- import
'''